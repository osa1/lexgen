#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LexerError<E> {
    InvalidToken {
        location: Loc,
    },

    /// Custom error, raised by a semantic action
    Custom(E),
}

/// A location, used in errors
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct Loc {
    pub line: u32,
    pub col: u32,
}

/// **Do not use**
// Possible outcomes of user action
pub enum SemanticActionResult<T> {
    // User action did not return a token, continue with lexing
    Continue,
    // User action returned a token, return it
    Return(T),
}

impl<T> SemanticActionResult<T> {
    pub fn map_token<F, T1>(self, f: F) -> SemanticActionResult<T1>
    where
        F: Fn(T) -> T1,
    {
        match self {
            SemanticActionResult::Continue => SemanticActionResult::Continue,
            SemanticActionResult::Return(t) => SemanticActionResult::Return(f(t)),
        }
    }
}

/// Common parts in lexers generated by lexgen.
///
/// **Fields are used by lexgen-generated code and should not be used directly.**
pub struct Lexer<'input, Token, State, Error, Wrapper> {
    // Current lexer state
    pub __state: usize,

    // Set after end-of-input is handled by a rule, or by default in `Init` rule
    pub __done: bool,

    // Which lexer state to switch to on successful match
    pub __initial_state: usize,

    pub __user_state: State,

    // User-provided input string. Does not change after initialization.
    pub __input: &'input str,

    // Start index of `iter`. We update this as we backtrack and update `iter`.
    pub __iter_byte_idx: usize,

    // Character iterator. `Peekable` is used in the handler's `peek` method. Note that we can't
    // use byte index returned by this directly, as we re-initialize this field when backtracking.
    // Add `iter_byte_idx` to the byte index before using. When resetting, update `iter_byte_idx`.
    pub __iter: std::iter::Peekable<std::str::CharIndices<'input>>,

    // Where does the current match start. Byte index in `input`.
    pub __current_match_start: usize,

    // Where does the current match end (exclusive). Byte index in `input`.
    pub __current_match_end: usize,

    // If we skipped an accepting state, this holds the triple: - Skipped match start (byte index
    // in `input`) - Semantic action (a function name) - Skipped match end (exclusive, byte index
    // in `input`)
    pub __last_match: Option<(
        usize,
        for<'lexer, 'input_> fn(&'lexer mut Wrapper) -> SemanticActionResult<Result<Token, Error>>,
        usize,
    )>,
}

impl<'input, T, S: Default, E, W> Lexer<'input, T, S, E, W> {
    pub fn new(input: &'input str) -> Self {
        Self {
            __state: 0,
            __done: false,
            __initial_state: 0,
            __user_state: Default::default(),
            __input: input,
            __iter_byte_idx: 0,
            __iter: input.char_indices().peekable(),
            __current_match_start: 0,
            __current_match_end: 0,
            __last_match: None,
        }
    }
}

impl<'input, T, S, E, W> Lexer<'input, T, S, E, W> {
    pub fn new_with_state(input: &'input str, state: S) -> Self {
        Self {
            __state: 0,
            __done: false,
            __initial_state: 0,
            __user_state: state,
            __input: input,
            __iter_byte_idx: 0,
            __iter: input.char_indices().peekable(),
            __current_match_start: 0,
            __current_match_end: 0,
            __last_match: None,
        }
    }

    // On success returns semantic action function for the last match
    pub fn backtrack(
        &mut self,
    ) -> Result<for<'lexer> fn(&'lexer mut W) -> SemanticActionResult<Result<T, E>>, LexerError<E>>
    {
        match self.__last_match.take() {
            // TODO: location
            None => Err(LexerError::InvalidToken {
                location: Loc { line: 0, col: 0 },
            }),
            Some((match_start, semantic_action, match_end)) => {
                self.__done = false;
                self.__current_match_start = match_start;
                self.__current_match_end = match_end;
                self.__iter = self.__input[match_end..].char_indices().peekable();
                self.__iter_byte_idx = match_end;
                Ok(semantic_action)
            }
        }
    }
}
